{
 "cells": [
  {
   "cell_type": "code",
   "id": "0100d4a9-75be-48f1-9e75-ae0085e70610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:33:45.532283Z",
     "start_time": "2025-05-23T16:33:45.227871Z"
    }
   },
   "source": [
    "def run_pandas():\n",
    "    \"\"\"Implementação completa com Pandas e medição de métricas.\"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.impute import KNNImputer\n",
    "    import time # Para medir o tempo de execução\n",
    "    import tracemalloc # Para medir a utilização de memória\n",
    "    import os # Para verificar o tamanho dos arquivos\n",
    "\n",
    "\n",
    "    # --- Tempo de Execução ---\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start() # Inicia o rastreamento de memória\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 1: Modelo de ML (ML_Pisa) completa os valores omissos do dataset pisa_2006-2018\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    pisa = pd.read_csv(\"pisa_2006-2018.csv\")\n",
    "    pisa.replace('NA', np.nan, inplace=True)\n",
    "    pisa['Score'] = pd.to_numeric(pisa['Score'], errors='coerce')\n",
    "\n",
    "    # Codificação\n",
    "    le = LabelEncoder()\n",
    "    pisa['Country_encoded'] = le.fit_transform(pisa['Country'])\n",
    "    pisa['Subject_encoded'] = le.fit_transform(pisa['Subject'])\n",
    "\n",
    "    # Modelo\n",
    "    train = pisa.dropna(subset=['Score'])\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(train[['Year', 'Subject_encoded', 'Country_encoded']], train['Score'])\n",
    "\n",
    "    # Previsão\n",
    "    missing = pisa[pisa['Score'].isna()]\n",
    "    if not missing.empty:\n",
    "        pred = model.predict(missing[['Year', 'Subject_encoded', 'Country_encoded']])\n",
    "        pisa.loc[pisa['Score'].isna(), 'Score'] = np.round(pred)\n",
    "\n",
    "    pisa.to_csv(\"pisa_imputed_pandas.csv\", index=False)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 2: Junção dos datasets\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Carregar os datasets\n",
    "    pisa_imputed = pd.read_csv(\"pisa_imputed_pandas.csv\")\n",
    "    gdp = pd.read_csv(\"GDP.csv\")\n",
    "\n",
    "    # Corrigir possíveis espaços nos nomes das colunas\n",
    "    gdp.columns = gdp.columns.str.strip()\n",
    "\n",
    "    # Filtrar apenas os anos de interesse no dataset PISA\n",
    "    pisa_imputed = pisa_imputed[pisa_imputed['Year'].isin([2012, 2015, 2018])]\n",
    "\n",
    "    # Filtrar colunas relevantes do GDP\n",
    "    gdp = gdp[['Country', 'Country Code', '2012', '2015', '2018']]\n",
    "\n",
    "    # Obter lista de países comuns\n",
    "    paises_comuns = set(pisa_imputed['Country']).intersection(set(gdp['Country']))\n",
    "\n",
    "    # Filtrar os datasets para manter apenas países comuns\n",
    "    gdp_filtrado = gdp[gdp['Country'].isin(paises_comuns)].copy()\n",
    "    pisa_filtrado = pisa_imputed[pisa_imputed['Country'].isin(paises_comuns)].copy()\n",
    "\n",
    "    # Transformar GDP de wide para long\n",
    "    gdp_long = gdp_filtrado.melt(\n",
    "        id_vars=['Country', 'Country Code'],\n",
    "        value_vars=['2012', '2015', '2018'],\n",
    "        var_name='Year',\n",
    "        value_name='GDP'\n",
    "    )\n",
    "\n",
    "    # Garantir que o ano está como inteiro\n",
    "    gdp_long['Year'] = gdp_long['Year'].astype(int)\n",
    "\n",
    "    # Transformar PISA de long para wide (Subject → colunas)\n",
    "    pisa_wide = pisa_filtrado[['Country', 'Year', 'Subject', 'Score']].pivot_table(\n",
    "        index=['Country', 'Year'],\n",
    "        columns='Subject',\n",
    "        values='Score'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Juntar os datasets\n",
    "    dados_combinados = pd.merge(gdp_long, pisa_wide, on=['Country', 'Year'], how='left')\n",
    "\n",
    "    # Reestruturar para wide por ano\n",
    "    pisa_e_GDP = dados_combinados.pivot(\n",
    "        index=['Country', 'Country Code'],\n",
    "        columns='Year',\n",
    "        values=['GDP', 'Maths', 'Science', 'Reading']\n",
    "    )\n",
    "\n",
    "    # Ajustar nomes das colunas para formato mais legível\n",
    "    pisa_e_GDP.columns = [f\"{var}_{year}\" for var, year in pisa_e_GDP.columns]\n",
    "    pisa_e_GDP = pisa_e_GDP.reset_index()\n",
    "    pisa_e_GDP = pisa_e_GDP.rename(columns={\n",
    "        'Country Code': 'Country_code'\n",
    "    })\n",
    "\n",
    "    pisa_e_GDP.to_csv(\"pisa_e_GDP.csv\", index=False)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 3: Processamento do GDP_PISA (ML_GDP_Pisa)\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    dt = pd.read_csv(\"pisa_e_GDP.csv\")\n",
    "\n",
    "    # Tratamento de NAs\n",
    "    print(\"\\n[Pandas] Valores NaN antes:\", dt.isna().sum().sum())\n",
    "\n",
    "    # Salvar máscara de valores ausentes para usar depois\n",
    "    missing_mask = dt.isna()\n",
    "\n",
    "    colunas_numericas = dt.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    dt_scaled = dt.copy()\n",
    "    dt_scaled[colunas_numericas] = scaler.fit_transform(dt[colunas_numericas])\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    dt_imputed_scaled = dt_scaled.copy()\n",
    "    dt_imputed_scaled[colunas_numericas] = imputer.fit_transform(dt_scaled[colunas_numericas])\n",
    "\n",
    "    dt_imputed = dt.copy()\n",
    "    dt_imputed[colunas_numericas] = scaler.inverse_transform(dt_imputed_scaled[colunas_numericas])\n",
    "\n",
    "    # Formatação\n",
    "    colunas_gdp = [\"GDP_2012\", \"GDP_2015\", \"GDP_2018\"]\n",
    "    dt_imputed[colunas_gdp] = dt_imputed[colunas_gdp].round(5)\n",
    "\n",
    "    # Correção: Aplicar arredondamento e conversão para int apenas nas células que eram NaN\n",
    "    # Exclui colunas GDP que já foram tratadas separadamente\n",
    "    colunas_para_arredondar = [col for col in colunas_numericas if col not in colunas_gdp]\n",
    "\n",
    "    for col in colunas_para_arredondar:\n",
    "        if missing_mask[col].any():\n",
    "            dt_imputed.loc[missing_mask[col], col] = dt_imputed.loc[missing_mask[col], col].round(0).astype(int)\n",
    "\n",
    "    print(\"[Pandas] Valores NaN após:\", dt_imputed.isna().sum().sum())\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 4: Adicionar as médias dos anos e das disciplinas\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Criar novas colunas com médias das disciplinas arredondadas a 3 casas decimais\n",
    "    dt_imputed['Means_Maths'] = dt_imputed[['Maths_2012', 'Maths_2015', 'Maths_2018']].mean(axis=1, skipna=True).round(3)\n",
    "    dt_imputed['Means_Science'] = dt_imputed[['Science_2012', 'Science_2015', 'Science_2018']].mean(axis=1, skipna=True).round(3)\n",
    "    dt_imputed['Means_Reading'] = dt_imputed[['Reading_2012', 'Reading_2015', 'Reading_2018']].mean(axis=1, skipna=True).round(3)\n",
    "\n",
    "    dt_imputed['Mean_2012'] = dt_imputed[[c for c in dt_imputed.columns if '2012' in c and 'GDP' not in c]].mean(axis=1).round(3)\n",
    "    dt_imputed['Mean_2015'] = dt_imputed[[c for c in dt_imputed.columns if '2015' in c and 'GDP' not in c]].mean(axis=1).round(3)\n",
    "    dt_imputed['Mean_2018'] = dt_imputed[[c for c in dt_imputed.columns if '2018' in c and 'GDP' not in c]].mean(axis=1).round(3)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 5: Adicionar variações relativas (%) entre anos\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Variação percentual do GDP\n",
    "    dt_imputed['GDP_change_2012_2015'] = ((dt_imputed['GDP_2015'] - dt_imputed['GDP_2012']) / dt_imputed['GDP_2012'] * 100).round(2)\n",
    "    dt_imputed['GDP_change_2015_2018'] = ((dt_imputed['GDP_2018'] - dt_imputed['GDP_2015']) / dt_imputed['GDP_2015'] * 100).round(2)\n",
    "\n",
    "    # Variação percentual das médias PISA\n",
    "    dt_imputed['PISA_change_2012_2015'] = ((dt_imputed['Mean_2015'] - dt_imputed['Mean_2012']) / dt_imputed['Mean_2012'] * 100).round(2)\n",
    "    dt_imputed['PISA_change_2015_2018'] = ((dt_imputed['Mean_2018'] - dt_imputed['Mean_2015']) / dt_imputed['Mean_2015'] * 100).round(2)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 6: Rácio GDP / PISA Score por ano\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    dt_imputed['GDP_per_PISA_2012'] = (dt_imputed['GDP_2012'] / dt_imputed['Mean_2012']).round(4)\n",
    "    dt_imputed['GDP_per_PISA_2015'] = (dt_imputed['GDP_2015'] / dt_imputed['Mean_2015']).round(4)\n",
    "    dt_imputed['GDP_per_PISA_2018'] = (dt_imputed['GDP_2018'] / dt_imputed['Mean_2018']).round(4)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 7: Diferença absoluta das pontuações PISA e do GDP entre anos\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Diferença absoluta\n",
    "    dt_imputed['GDP_diff_2012_2015'] = (dt_imputed['GDP_2015'] - dt_imputed['GDP_2012']).round(3)\n",
    "    dt_imputed['GDP_diff_2015_2018'] = (dt_imputed['GDP_2018'] - dt_imputed['GDP_2015']).round(3)\n",
    "\n",
    "    dt_imputed['PISA_diff_2012_2015'] = (dt_imputed['Mean_2015'] - dt_imputed['Mean_2012']).round(3)\n",
    "    dt_imputed['PISA_diff_2015_2018'] = (dt_imputed['Mean_2018'] - dt_imputed['Mean_2015']).round(3)\n",
    "\n",
    "    output_filename = \"Dataset_final_pandas.xlsx\"\n",
    "    dt_imputed.to_excel(output_filename, index=False)\n",
    "\n",
    "    # --- Medição do Tempo de Execução e Memória ---\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop() # Para o rastreamento de memória\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # --- Resultados das Métricas ---\n",
    "    print(f\"\\n--- Métricas de Desempenho ---\")\n",
    "    print(f\"Tempo de Execução: {execution_time:.4f} segundos\")\n",
    "    print(f\"Utilização de Memória Pico: {peak / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Tamanho do arquivo de saída ('{output_filename}'): {os.path.getsize(output_filename) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    return f\"Resultados gravados em '{output_filename}'\"\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "fb718d12-ec3d-4400-a547-7a0c1efdb5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T15:48:54.481434Z",
     "start_time": "2025-05-23T15:48:54.364458Z"
    }
   },
   "source": [
    "def run_pyspark():\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import DoubleType, IntegerType\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    from pyspark.ml.regression import RandomForestRegressor\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    from pyspark.ml.feature import Imputer, StandardScaler\n",
    "    from pyspark.sql.window import Window\n",
    "    import os # Para verificar o tamanho dos arquivos\n",
    "    # import tracemalloc # <-- Remova esta linha, pois não é adequada para o Spark JVM memory\n",
    "\n",
    "    # --- Tempo de Execução ---\n",
    "    start_time = time.time()\n",
    "    # tracemalloc.start() # <-- Remova esta linha\n",
    "\n",
    "    # Iniciar sessão Spark com configuração apropriada\n",
    "    # Ajuste 'spark.driver.memory' e 'spark.executor.memory' conforme necessário para o seu ambiente\n",
    "    # Vamos capturar a configuração da memória do driver aqui:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PISA_GDP_Analysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Captura a memória do driver configurada\n",
    "    spark_driver_memory = spark.conf.get(\"spark.driver.memory\")\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 1: Modelo de Machine Learning para Imputação de Dados PISA\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Carregar dataset PISA a partir de ficheiro CSV\n",
    "    pisa = spark.read.csv(\"pisa_2006-2018.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Converter coluna Score para tipo double e lidar com valores 'NA'\n",
    "    pisa = pisa.withColumn(\"Score\",\n",
    "                          F.when(F.col(\"Score\") == \"NA\", None)\n",
    "                          .otherwise(F.col(\"Score\").cast(DoubleType())))\n",
    "\n",
    "    # Preparar codificação categórica para colunas Country e Subject\n",
    "    countryIndexer = StringIndexer(inputCol=\"Country\", outputCol=\"Country_encoded\")\n",
    "    subjectIndexer = StringIndexer(inputCol=\"Subject\", outputCol=\"Subject_encoded\")\n",
    "\n",
    "    # Criar e aplicar pipeline de codificação\n",
    "    indexPipeline = Pipeline(stages=[countryIndexer, subjectIndexer])\n",
    "    indexModel = indexPipeline.fit(pisa)\n",
    "    pisaIndexed = indexModel.transform(pisa)\n",
    "\n",
    "    # Dividir dados em conjunto de treino (com scores) e conjunto para previsão (scores em falta)\n",
    "    train = pisaIndexed.filter(pisaIndexed.Score.isNotNull())\n",
    "    missing = pisaIndexed.filter(pisaIndexed.Score.isNull())\n",
    "\n",
    "    # Preparar vetor de features para modelo de machine learning\n",
    "    featureAssembler = VectorAssembler(\n",
    "        inputCols=[\"Year\", \"Subject_encoded\", \"Country_encoded\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Configurar e treinar modelo de regressão Random Forest\n",
    "    rf = RandomForestRegressor(\n",
    "        labelCol=\"Score\",\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=100,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[featureAssembler, rf])\n",
    "    model = pipeline.fit(train)\n",
    "\n",
    "    # Gerar previsões para scores em falta, se necessário\n",
    "    if missing.count() > 0:\n",
    "        predictions = model.transform(missing)\n",
    "        predictions = predictions.withColumn(\"Score\", F.round(F.col(\"prediction\")))\n",
    "\n",
    "        # Combinar dados originais com previsões\n",
    "        pisaImputed = train.select(\"Country\", \"Subject\", \"Year\", \"Score\") \\\n",
    "            .union(predictions.select(\"Country\", \"Subject\", \"Year\", \"Score\"))\n",
    "    else:\n",
    "        pisaImputed = train.select(\"Country\", \"Subject\", \"Year\", \"Score\")\n",
    "\n",
    "    # Guardar dataset imputado como CSV\n",
    "    pisaImputed.toPandas().to_csv(\"pisa_imputed_pyspark.csv\", index=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 2: Fusão dos Datasets PISA e PIB\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Carregar datasets processados\n",
    "    pisa_imputed = spark.read.csv(\"pisa_imputed_pyspark.csv\", header=True, inferSchema=True)\n",
    "    gdp = spark.read.csv(\"GDP.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Limpar nomes de colunas removendo espaços\n",
    "    for col_name in gdp.columns:\n",
    "        if ' ' in col_name:\n",
    "            gdp = gdp.withColumnRenamed(col_name, col_name.strip())\n",
    "\n",
    "    # Filtrar dados PISA para anos específicos de interesse\n",
    "    pisa_imputed = pisa_imputed.filter(F.col(\"Year\").isin([2012, 2015, 2018]))\n",
    "\n",
    "    # Selecionar colunas relevantes dos dados PIB\n",
    "    gdp = gdp.select(\"Country\", \"Country Code\", \"2012\", \"2015\", \"2018\")\n",
    "\n",
    "    # Identificar países comuns entre ambos os datasets\n",
    "    paises_pisa = set([row[\"Country\"] for row in pisa_imputed.select(\"Country\").distinct().collect()])\n",
    "    paises_gdp = set([row[\"Country\"] for row in gdp.select(\"Country\").distinct().collect()])\n",
    "    paises_comuns = paises_pisa.intersection(paises_gdp)\n",
    "\n",
    "    # Filtrar datasets para manter apenas países comuns\n",
    "    gdp_filtrado = gdp.filter(F.col(\"Country\").isin(list(paises_comuns)))\n",
    "    pisa_filtrado = pisa_imputed.filter(F.col(\"Country\").isin(list(paises_comuns)))\n",
    "\n",
    "    # Reformatar dados PIB de formato largo para longo\n",
    "    gdp_long = gdp_filtrado.select(\n",
    "        \"Country\",\n",
    "        F.col(\"Country Code\").alias(\"Country_Code\"),\n",
    "        F.lit(\"2012\").alias(\"Year\"),\n",
    "        F.col(\"2012\").alias(\"GDP\")\n",
    "    ).union(\n",
    "        gdp_filtrado.select(\n",
    "            \"Country\",\n",
    "            F.col(\"Country Code\").alias(\"Country_Code\"),\n",
    "            F.lit(\"2015\").alias(\"Year\"),\n",
    "            F.col(\"2015\").alias(\"GDP\")\n",
    "        )\n",
    "    ).union(\n",
    "        gdp_filtrado.select(\n",
    "            \"Country\",\n",
    "            F.col(\"Country Code\").alias(\"Country_Code\"),\n",
    "            F.lit(\"2018\").alias(\"Year\"),\n",
    "            F.col(\"2018\").alias(\"GDP\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Garantir que a coluna Year tem o tipo inteiro correto\n",
    "    gdp_long = gdp_long.withColumn(\"Year\", F.col(\"Year\").cast(IntegerType()))\n",
    "\n",
    "    # Reformatar dados PISA de formato longo para largo (disciplinas como colunas)\n",
    "    pisa_pivot = pisa_filtrado.groupBy(\"Country\", \"Year\").pivot(\"Subject\").agg(F.first(\"Score\"))\n",
    "\n",
    "    # Combinar ambos os datasets em Country e Year\n",
    "    dados_combinados = gdp_long.join(pisa_pivot, on=[\"Country\", \"Year\"], how=\"left\")\n",
    "\n",
    "    # Reestruturar dados para formato largo por ano\n",
    "    dados_wide = dados_combinados\n",
    "\n",
    "    # Criar colunas específicas por ano para PIB e cada disciplina\n",
    "    for year in [2012, 2015, 2018]:\n",
    "        dados_wide = dados_wide.withColumn(\n",
    "            f\"GDP_{year}\",\n",
    "            F.when(F.col(\"Year\") == year, F.col(\"GDP\")).otherwise(None)\n",
    "        )\n",
    "\n",
    "        for subject in [\"Maths\", \"Science\", \"Reading\"]:\n",
    "            dados_wide = dados_wide.withColumn(\n",
    "                f\"{subject}_{year}\",\n",
    "                F.when(F.col(\"Year\") == year, F.col(subject)).otherwise(None)\n",
    "            )\n",
    "\n",
    "    # Agregar por país para consolidar todos os valores\n",
    "    pisa_e_GDP = dados_wide.groupBy(\"Country\", \"Country_Code\").agg(\n",
    "        F.max(\"GDP_2012\").alias(\"GDP_2012\"),\n",
    "        F.max(\"GDP_2015\").alias(\"GDP_2015\"),\n",
    "        F.max(\"GDP_2018\").alias(\"GDP_2018\"),\n",
    "        F.max(\"Maths_2012\").alias(\"Maths_2012\"),\n",
    "        F.max(\"Maths_2015\").alias(\"Maths_2015\"),\n",
    "        F.max(\"Maths_2018\").alias(\"Maths_2018\"),\n",
    "        F.max(\"Science_2012\").alias(\"Science_2012\"),\n",
    "        F.max(\"Science_2015\").alias(\"Science_2015\"),\n",
    "        F.max(\"Science_2018\").alias(\"Science_2018\"),\n",
    "        F.max(\"Reading_2012\").alias(\"Reading_2012\"),\n",
    "        F.max(\"Reading_2015\").alias(\"Reading_2015\"),\n",
    "        F.max(\"Reading_2018\").alias(\"Reading_2018\")\n",
    "    )\n",
    "\n",
    "    # Padronizar nomes de colunas\n",
    "    pisa_e_GDP = pisa_e_GDP.withColumnRenamed(\"Country_Code\", \"Country_code\")\n",
    "\n",
    "    # Guardar dataset combinado\n",
    "    pisa_e_GDP.toPandas().to_csv(\"pisa_e_GDP_pyspark.csv\", index=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 3: Processamento e Imputação de Dados\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Carregar dataset combinado\n",
    "    dt = spark.read.csv(\"pisa_e_GDP_pyspark.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Contar valores em falta antes da imputação\n",
    "    nan_count_before = sum([dt.filter(F.col(c).isNull()).count() for c in dt.columns])\n",
    "    print(f\"\\n[PySpark] Valores NaN antes do processamento: {nan_count_before}\")\n",
    "\n",
    "    # Identificar colunas numéricas para processamento\n",
    "    colunas_numericas = [field.name for field in dt.schema.fields\n",
    "                          if isinstance(field.dataType, (DoubleType, IntegerType))]\n",
    "\n",
    "    # Lidar com valores em falta usando imputação (substituindo o KNNImputer do Pandas)\n",
    "    imputer = Imputer(\n",
    "        inputCols=colunas_numericas,\n",
    "        outputCols=[f\"{c}\" for c in colunas_numericas], # Sobrescreve as colunas originais\n",
    "        strategy=\"mean\"\n",
    "    )\n",
    "\n",
    "    imputer_model = imputer.fit(dt)\n",
    "    dt_imputed = imputer_model.transform(dt)\n",
    "\n",
    "    # Formatar colunas PIB para 5 casas decimais\n",
    "    colunas_gdp = [\"GDP_2012\", \"GDP_2015\", \"GDP_2018\"]\n",
    "    for col in colunas_gdp:\n",
    "        dt_imputed = dt_imputed.withColumn(col, F.round(F.col(col), 5))\n",
    "\n",
    "    # Aplicar arredondamento e conversão de tipo apenas a valores que seriam imputados (se fossem nulos originalmente)\n",
    "    colunas_para_arredondar = [col for col in colunas_numericas if col not in colunas_gdp]\n",
    "\n",
    "    for col in colunas_para_arredondar:\n",
    "        dt_imputed = dt_imputed.withColumn(col, F.round(F.col(col), 0).cast(IntegerType()))\n",
    "\n",
    "    # Contar valores em falta restantes após processamento\n",
    "    nan_count_after = sum([dt_imputed.filter(F.col(c).isNull()).count() for c in dt_imputed.columns])\n",
    "    print(f\"[PySpark] Valores NaN após processamento: {nan_count_after}\")\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 4: Cálculo de Médias por Ano e Disciplina\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Calcular médias de disciplinas ao longo dos anos\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Means_Maths\",\n",
    "        F.round(F.array_avg(F.array(F.col(\"Maths_2012\"), F.col(\"Maths_2015\"), F.col(\"Maths_2018\"))), 3)\n",
    "    )\n",
    "\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Means_Science\",\n",
    "        F.round(F.array_avg(F.array(F.col(\"Science_2012\"), F.col(\"Science_2015\"), F.col(\"Science_2018\"))), 3)\n",
    "    )\n",
    "\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Means_Reading\",\n",
    "        F.round(F.array_avg(F.array(F.col(\"Reading_2012\"), F.col(\"Reading_2015\"), F.col(\"Reading_2018\"))), 3)\n",
    "    )\n",
    "\n",
    "    # Calcular médias anuais por disciplina\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Mean_2012\",\n",
    "        F.round(F.array_avg(F.array(F.col(\"Maths_2012\"), F.col(\"Science_2012\"), F.col(\"Reading_2012\"))), 3)\n",
    "    )\n",
    "\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Mean_2015\",\n",
    "        F.round(F.array_avg(F.array(F.col(\"Maths_2015\"), F.col(\"Science_2015\"), F.col(\"Reading_2015\"))), 3)\n",
    "    )\n",
    "\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Mean_2018\",\n",
    "        F.round(F.array_avg(F.array(F.col(\"Maths_2018\"), F.col(\"Science_2018\"), F.col(\"Reading_2018\"))), 3)\n",
    "    )\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 5: Cálculo de Variações Percentuais Entre Anos\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Calcular variações percentuais do PIB\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_change_2012_2015',\n",
    "        F.round(((F.col('GDP_2015') - F.col('GDP_2012')) / F.col('GDP_2012') * 100), 2)\n",
    "    )\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_change_2015_2018',\n",
    "        F.round(((F.col('GDP_2018') - F.col('GDP_2015')) / F.col('GDP_2015') * 100), 2)\n",
    "    )\n",
    "\n",
    "    # Calcular variações percentuais dos scores PISA\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'PISA_change_2012_2015',\n",
    "        F.round(((F.col('Mean_2015') - F.col('Mean_2012')) / F.col('Mean_2012') * 100), 2)\n",
    "    )\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'PISA_change_2015_2018',\n",
    "        F.round(((F.col('Mean_2018') - F.col('Mean_2015')) / F.col('Mean_2015') * 100), 2)\n",
    "    )\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 6: Cálculo de Rácios PIB por Score PISA\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_per_PISA_2012',\n",
    "        F.round((F.col('GDP_2012') / F.col('Mean_2012')), 4)\n",
    "    )\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_per_PISA_2015',\n",
    "        F.round((F.col('GDP_2015') / F.col('Mean_2015')), 4)\n",
    "    )\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_per_PISA_2018',\n",
    "        F.round((F.col('GDP_2018') / F.col('Mean_2018')), 4)\n",
    "    )\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 7: Cálculo de Diferenças Absolutas Entre Anos\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Calcular diferenças absolutas do PIB\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_diff_2012_2015',\n",
    "        F.round((F.col('GDP_2015') - F.col('GDP_2012')), 3)\n",
    "    )\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'GDP_diff_2015_2018',\n",
    "        F.round((F.col('GDP_2018') - F.col('GDP_2015')), 3)\n",
    "    )\n",
    "\n",
    "    # Calcular diferenças absolutas dos scores PISA\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'PISA_diff_2012_2015',\n",
    "        F.round((F.col('Mean_2015') - F.col('Mean_2012')), 3)\n",
    "    )\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        'PISA_diff_2015_2018',\n",
    "        F.round((F.col('Mean_2018') - F.col('Mean_2015')), 3)\n",
    "    )\n",
    "\n",
    "    output_filename = \"Dataset_final_pyspark.xlsx\"\n",
    "    dt_imputed.toPandas().to_excel(output_filename, index=False)\n",
    "\n",
    "    # Limpar terminando a sessão Spark\n",
    "    spark.stop()\n",
    "\n",
    "    # --- Medição do Tempo de Execução e Tamanho do Arquivo ---\n",
    "    # current, peak = tracemalloc.get_traced_memory() # <-- Remova esta linha\n",
    "    # tracemalloc.stop() # <-- Remova esta linha\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # --- Resultados das Métricas ---\n",
    "    print(f\"\\n--- Métricas de Desempenho (PySpark) ---\")\n",
    "    print(f\"Tempo de Execução: {execution_time:.4f} segundos\")\n",
    "    # Para a memória, reportamos a memória configurada do driver, que é a capacidade principal para este dataset pequeno.\n",
    "    print(f\"Memória do Driver Spark Configurada: {spark_driver_memory}\") # <-- Adiciona esta linha\n",
    "    print(f\"Tamanho do arquivo de saída ('{output_filename}'): {os.path.getsize(output_filename) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    return f\"Resultados gravados em '{output_filename}'\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "2d6a2eb9-2268-4874-ba32-5f0c273ed733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:57:54.302225Z",
     "start_time": "2025-05-23T17:57:52.887026Z"
    }
   },
   "source": [
    "run_pandas()\n",
    "\n",
    "run_pyspark()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pandas] Valores NaN antes: 70\n",
      "[Pandas] Valores NaN após: 0\n",
      "\n",
      "--- Métricas de Desempenho ---\n",
      "Tempo de Execução: 1.3988 segundos\n",
      "Utilização de Memória Pico: 1.75 MB\n",
      "Tamanho do arquivo de saída ('Dataset_final_pandas.xlsx'): 0.02 MB\n",
      "\n",
      "[PySpark] Valores NaN antes do processamento: 70\n",
      "[PySpark] Valores NaN após processamento: 0\n",
      "\n",
      "--- Métricas de Desempenho (PySpark) ---\n",
      "Tempo de Execução: 5.8500 segundos\n",
      "Memória do Driver Spark Configurada: 4g\n",
      "Tamanho do arquivo de saída (\"Dataset_final_pyspark.xlsx\"): 0.02 MB\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
