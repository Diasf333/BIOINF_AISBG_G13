{
 "cells": [
  {
   "cell_type": "code",
   "id": "0100d4a9-75be-48f1-9e75-ae0085e70610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:36.057266Z",
     "start_time": "2025-05-23T14:55:36.023117Z"
    }
   },
   "source": [
    "def run_pandas():\n",
    "    \"\"\"Implementação completa com Pandas\"\"\"\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.impute import KNNImputer\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 1: Modelo de ML (ML_Pisa) completa os valores omissos do dataset pisa_2006-2018\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    pisa = pd.read_csv(\"pisa_2006-2018.csv\")\n",
    "    pisa.replace('NA', np.nan, inplace=True)\n",
    "    pisa['Score'] = pd.to_numeric(pisa['Score'], errors='coerce')\n",
    "\n",
    "    # Codificação\n",
    "    le = LabelEncoder()\n",
    "    pisa['Country_encoded'] = le.fit_transform(pisa['Country'])\n",
    "    pisa['Subject_encoded'] = le.fit_transform(pisa['Subject'])\n",
    "\n",
    "    # Modelo\n",
    "    train = pisa.dropna(subset=['Score'])\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(train[['Year', 'Subject_encoded', 'Country_encoded']], train['Score'])\n",
    "\n",
    "    # Previsão\n",
    "    missing = pisa[pisa['Score'].isna()]\n",
    "    if not missing.empty:\n",
    "        pred = model.predict(missing[['Year', 'Subject_encoded', 'Country_encoded']])\n",
    "        pisa.loc[pisa['Score'].isna(), 'Score'] = np.round(pred)\n",
    "\n",
    "    pisa.to_csv(\"pisa_imputed_pandas.csv\", index=False)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 2: Junção dos datasets\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Carregar os datasets\n",
    "    pisa_imputed = pd.read_csv(\"pisa_imputed_pandas.csv\")\n",
    "    gdp = pd.read_csv(\"GDP.csv\")\n",
    "\n",
    "    # Corrigir possíveis espaços nos nomes das colunas\n",
    "    gdp.columns = gdp.columns.str.strip()\n",
    "\n",
    "    # Filtrar apenas os anos de interesse no dataset PISA\n",
    "    pisa_imputed = pisa_imputed[pisa_imputed['Year'].isin([2012, 2015, 2018])]\n",
    "\n",
    "    # Filtrar colunas relevantes do GDP\n",
    "    gdp = gdp[['Country', 'Country Code', '2012', '2015', '2018']]\n",
    "\n",
    "    # Obter lista de países comuns\n",
    "    paises_comuns = set(pisa_imputed['Country']).intersection(set(gdp['Country']))\n",
    "\n",
    "    # Filtrar os datasets para manter apenas países comuns\n",
    "    gdp_filtrado = gdp[gdp['Country'].isin(paises_comuns)].copy()\n",
    "    pisa_filtrado = pisa_imputed[pisa_imputed['Country'].isin(paises_comuns)].copy()\n",
    "\n",
    "    # Transformar GDP de wide para long\n",
    "    gdp_long = gdp_filtrado.melt(\n",
    "        id_vars=['Country', 'Country Code'],\n",
    "        value_vars=['2012', '2015', '2018'],\n",
    "        var_name='Year',\n",
    "        value_name='GDP'\n",
    "    )\n",
    "\n",
    "    # Garantir que o ano está como inteiro\n",
    "    gdp_long['Year'] = gdp_long['Year'].astype(int)\n",
    "\n",
    "    # Transformar PISA de long para wide (Subject → colunas)\n",
    "    pisa_wide = pisa_filtrado[['Country', 'Year', 'Subject', 'Score']].pivot_table(\n",
    "        index=['Country', 'Year'],\n",
    "        columns='Subject',\n",
    "        values='Score'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Juntar os datasets\n",
    "    dados_combinados = pd.merge(gdp_long, pisa_wide, on=['Country', 'Year'], how='left')\n",
    "\n",
    "    # Reestruturar para wide por ano\n",
    "    pisa_e_GDP = dados_combinados.pivot(\n",
    "        index=['Country', 'Country Code'],\n",
    "        columns='Year',\n",
    "        values=['GDP', 'Maths', 'Science', 'Reading']\n",
    "    )\n",
    "\n",
    "    # Ajustar nomes das colunas para formato mais legível\n",
    "    pisa_e_GDP.columns = [f\"{var}_{year}\" for var, year in pisa_e_GDP.columns]\n",
    "    pisa_e_GDP = pisa_e_GDP.reset_index()\n",
    "    pisa_e_GDP = pisa_e_GDP.rename(columns={\n",
    "        'Country Code': 'Country_code'\n",
    "    })\n",
    "\n",
    "    pisa_e_GDP.to_csv(\"pisa_e_GDP.csv\", index=False)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 3: Processamento do GDP_PISA (ML_GDP_Pisa)\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    dt = pd.read_csv(\"pisa_e_GDP.csv\")\n",
    "\n",
    "    # Tratamento de NAs\n",
    "    print(\"\\n[Pandas] Valores NaN antes:\", dt.isna().sum().sum())\n",
    "\n",
    "    # Salvar máscara de valores ausentes para usar depois\n",
    "    missing_mask = dt.isna()\n",
    "\n",
    "    colunas_numericas = dt.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    dt_scaled = dt.copy()\n",
    "    dt_scaled[colunas_numericas] = scaler.fit_transform(dt[colunas_numericas])\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    dt_imputed_scaled = dt_scaled.copy()\n",
    "    dt_imputed_scaled[colunas_numericas] = imputer.fit_transform(dt_scaled[colunas_numericas])\n",
    "\n",
    "    dt_imputed = dt.copy()\n",
    "    dt_imputed[colunas_numericas] = scaler.inverse_transform(dt_imputed_scaled[colunas_numericas])\n",
    "\n",
    "    # Formatação\n",
    "    colunas_gdp = [\"GDP_2012\", \"GDP_2015\", \"GDP_2018\"]\n",
    "    dt_imputed[colunas_gdp] = dt_imputed[colunas_gdp].round(5)\n",
    "\n",
    "    # Correção: Aplicar arredondamento e conversão para int apenas nas células que eram NaN\n",
    "    # Exclui colunas GDP que já foram tratadas separadamente\n",
    "    colunas_para_arredondar = [col for col in colunas_numericas if col not in colunas_gdp]\n",
    "\n",
    "    for col in colunas_para_arredondar:\n",
    "        if missing_mask[col].any():\n",
    "            dt_imputed.loc[missing_mask[col], col] = dt_imputed.loc[missing_mask[col], col].round(0).astype(int)\n",
    "\n",
    "    print(\"[Pandas] Valores NaN após:\", dt_imputed.isna().sum().sum())\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 4: Adicionar as médias dos anos e das disciplinas\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Criar novas colunas com médias das disciplinas arredondadas a 3 casas decimais\n",
    "    dt_imputed['Means_Maths'] = dt_imputed[['Maths_2012', 'Maths_2015', 'Maths_2018']].mean(axis=1, skipna=True).round(3)\n",
    "    dt_imputed['Means_Science'] = dt_imputed[['Science_2012', 'Science_2015', 'Science_2018']].mean(axis=1, skipna=True).round(3)\n",
    "    dt_imputed['Means_Reading'] = dt_imputed[['Reading_2012', 'Reading_2015', 'Reading_2018']].mean(axis=1, skipna=True).round(3)\n",
    "\n",
    "    dt_imputed['Mean_2012'] = dt_imputed[[c for c in dt_imputed.columns if '2012' in c and 'GDP' not in c]].mean(axis=1).round(3)\n",
    "    dt_imputed['Mean_2015'] = dt_imputed[[c for c in dt_imputed.columns if '2015' in c and 'GDP' not in c]].mean(axis=1).round(3)\n",
    "    dt_imputed['Mean_2018'] = dt_imputed[[c for c in dt_imputed.columns if '2018' in c and 'GDP' not in c]].mean(axis=1).round(3)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 5: Adicionar variações relativas (%) entre anos\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Variação percentual do GDP\n",
    "    dt_imputed['GDP_change_2012_2015'] = ((dt_imputed['GDP_2015'] - dt_imputed['GDP_2012']) / dt_imputed['GDP_2012'] * 100).round(2)\n",
    "    dt_imputed['GDP_change_2015_2018'] = ((dt_imputed['GDP_2018'] - dt_imputed['GDP_2015']) / dt_imputed['GDP_2015'] * 100).round(2)\n",
    "\n",
    "    # Variação percentual das médias PISA\n",
    "    dt_imputed['PISA_change_2012_2015'] = ((dt_imputed['Mean_2015'] - dt_imputed['Mean_2012']) / dt_imputed['Mean_2012'] * 100).round(2)\n",
    "    dt_imputed['PISA_change_2015_2018'] = ((dt_imputed['Mean_2018'] - dt_imputed['Mean_2015']) / dt_imputed['Mean_2015'] * 100).round(2)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 6: Rácio GDP / PISA Score por ano\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    dt_imputed['GDP_per_PISA_2012'] = (dt_imputed['GDP_2012'] / dt_imputed['Mean_2012']).round(4)\n",
    "    dt_imputed['GDP_per_PISA_2015'] = (dt_imputed['GDP_2015'] / dt_imputed['Mean_2015']).round(4)\n",
    "    dt_imputed['GDP_per_PISA_2018'] = (dt_imputed['GDP_2018'] / dt_imputed['Mean_2018']).round(4)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 7: Diferença absoluta das pontuações PISA e do GDP entre anos\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Diferença absoluta\n",
    "    dt_imputed['GDP_diff_2012_2015'] = (dt_imputed['GDP_2015'] - dt_imputed['GDP_2012']).round(3)\n",
    "    dt_imputed['GDP_diff_2015_2018'] = (dt_imputed['GDP_2018'] - dt_imputed['GDP_2015']).round(3)\n",
    "\n",
    "    dt_imputed['PISA_diff_2012_2015'] = (dt_imputed['Mean_2015'] - dt_imputed['Mean_2012']).round(3)\n",
    "    dt_imputed['PISA_diff_2015_2018'] = (dt_imputed['Mean_2018'] - dt_imputed['Mean_2015']).round(3)\n",
    "\n",
    "    dt_imputed.to_excel(\"Dataset_final_pandas.xlsx\", index=False)\n",
    "\n",
    "\n",
    "    return f\"Tempo de execução Pandas: {time.time() - start_time} segundos\"\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fb718d12-ec3d-4400-a547-7a0c1efdb5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:54:58.137696Z",
     "start_time": "2025-05-23T14:54:58.091143Z"
    }
   },
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "# Usando o PySpark\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "def run_pyspark():\n",
    "    \"\"\"Implementação completa com PySpark\"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import DoubleType, IntegerType\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    from pyspark.ml.regression import RandomForestRegressor\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iniciar sessão Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PISA_GDP_Analysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 1: Modelo de ML (ML_Pisa)\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Carregar dados\n",
    "    pisa = spark.read.csv(\"pisa_2006-2018.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # Converter Score para double e substituir 'NA' por null\n",
    "    pisa = pisa.withColumn(\"Score\", \n",
    "                          F.when(F.col(\"Score\") == \"NA\", None)\n",
    "                          .otherwise(F.col(\"Score\").cast(DoubleType())))\n",
    "    \n",
    "    # Codificação com StringIndexer\n",
    "    countryIndexer = StringIndexer(inputCol=\"Country\", outputCol=\"Country_encoded\")\n",
    "    subjectIndexer = StringIndexer(inputCol=\"Subject\", outputCol=\"Subject_encoded\")\n",
    "    \n",
    "    # Criar pipeline para codificação\n",
    "    indexPipeline = Pipeline(stages=[countryIndexer, subjectIndexer])\n",
    "    indexModel = indexPipeline.fit(pisa)\n",
    "    pisaIndexed = indexModel.transform(pisa)\n",
    "    \n",
    "    # Dividir dados para treino (sem NaN) e previsão (com NaN)\n",
    "    train = pisaIndexed.filter(pisaIndexed.Score.isNotNull())\n",
    "    missing = pisaIndexed.filter(pisaIndexed.Score.isNull())\n",
    "    \n",
    "    # Preparar features para o modelo\n",
    "    featureAssembler = VectorAssembler(\n",
    "        inputCols=[\"Year\", \"Subject_encoded\", \"Country_encoded\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    # Criar e treinar o modelo\n",
    "    rf = RandomForestRegressor(\n",
    "        labelCol=\"Score\",\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=100,\n",
    "        seed=42,\n",
    "        maxDepth=10\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[featureAssembler, rf])\n",
    "    model = pipeline.fit(train)\n",
    "    \n",
    "    # Fazer previsões para valores em falta\n",
    "    if missing.count() > 0:\n",
    "        predictions = model.transform(missing)\n",
    "        predictions = predictions.withColumn(\"Score\", F.round(F.col(\"prediction\")))\n",
    "        \n",
    "        # Unir os dados treinados com as previsões\n",
    "        pisaImputed = train.select(\"Country\", \"Subject\", \"Year\", \"Score\") \\\n",
    "            .union(predictions.select(\"Country\", \"Subject\", \"Year\", \"Score\"))\n",
    "    else:\n",
    "        pisaImputed = train.select(\"Country\", \"Subject\", \"Year\", \"Score\")\n",
    "    \n",
    "    # Salvar o dataset imputado\n",
    "    pisaImputed.write.mode(\"overwrite\").csv(\"pisa_imputed_pyspark\", header=True)\n",
    "    \n",
    "    # Para compatibilidade com os próximos passos, vamos criar um arquivo CSV \n",
    "    # (normalmente não seria necessário em um pipeline completo Spark)\n",
    "    pisaImputed.toPandas().to_csv(\"pisa_imputed_pyspark.csv\", index=False)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 2: Junção dos datasets\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Carregar os datasets\n",
    "    pisa_imputed = spark.read.csv(\"pisa_imputed_pyspark.csv\", header=True, inferSchema=True)\n",
    "    gdp = spark.read.csv(\"GDP.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # Remover espaços nos nomes das colunas (PySpark não tem um método direto como o pandas)\n",
    "    for col_name in gdp.columns:\n",
    "        if ' ' in col_name:\n",
    "            gdp = gdp.withColumnRenamed(col_name, col_name.strip())\n",
    "    \n",
    "    # Filtrar apenas os anos de interesse no dataset PISA\n",
    "    pisa_imputed = pisa_imputed.filter(F.col(\"Year\").isin([2012, 2015, 2018]))\n",
    "    \n",
    "    # Selecionar colunas relevantes do GDP\n",
    "    gdp = gdp.select(\"Country\", \"Country Code\", \"2012\", \"2015\", \"2018\")\n",
    "    \n",
    "    # Obter lista de países comuns\n",
    "    paises_pisa = set([row[\"Country\"] for row in pisa_imputed.select(\"Country\").distinct().collect()])\n",
    "    paises_gdp = set([row[\"Country\"] for row in gdp.select(\"Country\").distinct().collect()])\n",
    "    paises_comuns = paises_pisa.intersection(paises_gdp)\n",
    "    \n",
    "    # Filtrar os datasets para manter apenas países comuns\n",
    "    gdp_filtrado = gdp.filter(F.col(\"Country\").isin(list(paises_comuns)))\n",
    "    pisa_filtrado = pisa_imputed.filter(F.col(\"Country\").isin(list(paises_comuns)))\n",
    "    \n",
    "    # Transformar GDP de wide para long\n",
    "    gdp_long = gdp_filtrado.select(\n",
    "        \"Country\", \n",
    "        F.col(\"Country Code\").alias(\"Country_Code\"),\n",
    "        F.lit(\"2012\").alias(\"Year\"), \n",
    "        F.col(\"2012\").alias(\"GDP\")\n",
    "    ).union(\n",
    "        gdp_filtrado.select(\n",
    "            \"Country\", \n",
    "            F.col(\"Country Code\").alias(\"Country_Code\"),\n",
    "            F.lit(\"2015\").alias(\"Year\"), \n",
    "            F.col(\"2015\").alias(\"GDP\")\n",
    "        )\n",
    "    ).union(\n",
    "        gdp_filtrado.select(\n",
    "            \"Country\", \n",
    "            F.col(\"Country Code\").alias(\"Country_Code\"),\n",
    "            F.lit(\"2018\").alias(\"Year\"), \n",
    "            F.col(\"2018\").alias(\"GDP\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Garantir que o ano está como inteiro\n",
    "    gdp_long = gdp_long.withColumn(\"Year\", F.col(\"Year\").cast(IntegerType()))\n",
    "    \n",
    "    # Transformar PISA de long para wide (Subject → colunas)\n",
    "    # Primeiro agrupar por País e Ano\n",
    "    pisa_pivot = pisa_filtrado.groupBy(\"Country\", \"Year\").pivot(\"Subject\").agg(F.first(\"Score\"))\n",
    "    \n",
    "    # Juntar os datasets\n",
    "    dados_combinados = gdp_long.join(pisa_pivot, on=[\"Country\", \"Year\"], how=\"left\")\n",
    "    \n",
    "    # Reestruturar para wide por ano (usando expressões mais complexas no PySpark)\n",
    "    # Primeiro criar uma coluna para cada combinação \"variável_ano\"\n",
    "    dados_wide = dados_combinados\n",
    "    \n",
    "    # Criar colunas GDP por ano\n",
    "    for year in [2012, 2015, 2018]:\n",
    "        dados_wide = dados_wide.withColumn(\n",
    "            f\"GDP_{year}\", \n",
    "            F.when(F.col(\"Year\") == year, F.col(\"GDP\")).otherwise(None)\n",
    "        )\n",
    "        \n",
    "        # Criar colunas para cada matéria e ano\n",
    "        for subject in [\"Maths\", \"Science\", \"Reading\"]:\n",
    "            dados_wide = dados_wide.withColumn(\n",
    "                f\"{subject}_{year}\", \n",
    "                F.when(F.col(\"Year\") == year, F.col(subject)).otherwise(None)\n",
    "            )\n",
    "    \n",
    "    # Agrupar por país para colocar os valores nos mesmos registros\n",
    "    window_spec = Window.partitionBy(\"Country\", \"Country_Code\")\n",
    "    \n",
    "    pisa_e_GDP = dados_wide.groupBy(\"Country\", \"Country_Code\").agg(\n",
    "        F.max(\"GDP_2012\").alias(\"GDP_2012\"),\n",
    "        F.max(\"GDP_2015\").alias(\"GDP_2015\"),\n",
    "        F.max(\"GDP_2018\").alias(\"GDP_2018\"),\n",
    "        F.max(\"Maths_2012\").alias(\"Maths_2012\"),\n",
    "        F.max(\"Maths_2015\").alias(\"Maths_2015\"),\n",
    "        F.max(\"Maths_2018\").alias(\"Maths_2018\"),\n",
    "        F.max(\"Science_2012\").alias(\"Science_2012\"),\n",
    "        F.max(\"Science_2015\").alias(\"Science_2015\"),\n",
    "        F.max(\"Science_2018\").alias(\"Science_2018\"),\n",
    "        F.max(\"Reading_2012\").alias(\"Reading_2012\"),\n",
    "        F.max(\"Reading_2015\").alias(\"Reading_2015\"),\n",
    "        F.max(\"Reading_2018\").alias(\"Reading_2018\")\n",
    "    )\n",
    "    \n",
    "    # Renomear colunas\n",
    "    pisa_e_GDP = pisa_e_GDP.withColumnRenamed(\"Country\", \"Nacao\").withColumnRenamed(\"Country_Code\", \"Country_code\")\n",
    "    \n",
    "    # Salvar como CSV\n",
    "    pisa_e_GDP.toPandas().to_csv(\"pisa_e_GDP_pyspark.csv\", index=False)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 3: Processamento do GDP_PISA (ML_GDP_Pisa)\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Carregar dados\n",
    "    dt = spark.read.csv(\"pisa_e_GDP_pyspark.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # Contar valores NaN antes da imputação\n",
    "    nan_count_before = sum([dt.filter(F.col(c).isNull()).count() for c in dt.columns])\n",
    "    print(f\"\\n[PySpark] Valores NaN antes: {nan_count_before}\")\n",
    "    \n",
    "    # Identificar colunas numéricas\n",
    "    colunas_numericas = [field.name for field in dt.schema.fields \n",
    "                          if isinstance(field.dataType, (DoubleType, IntegerType))]\n",
    "    \n",
    "    # Criar uma cópia do DataFrame com as colunas originais para depois recuperar\n",
    "    dt_original = dt\n",
    "    \n",
    "    # Padronização\n",
    "    assembler = VectorAssembler(inputCols=colunas_numericas, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    scalerModel = pipeline.fit(dt)\n",
    "    dt_scaled = scalerModel.transform(dt)\n",
    "    \n",
    "    # Imputação de valores ausentes (PySpark não tem KNNImputer, usando Imputer simples)\n",
    "    imputer = Imputer(\n",
    "        inputCols=colunas_numericas,\n",
    "        outputCols=[f\"{c}_imputed\" for c in colunas_numericas],\n",
    "        strategy=\"mean\"  # Usando média como estratégia\n",
    "    )\n",
    "    \n",
    "    imputer_model = imputer.fit(dt)\n",
    "    dt_imputed = imputer_model.transform(dt)\n",
    "    \n",
    "    # Substituir colunas originais pelas imputadas\n",
    "    for col in colunas_numericas:\n",
    "        dt_imputed = dt_imputed.withColumn(col, F.col(f\"{col}_imputed\"))\n",
    "        dt_imputed = dt_imputed.drop(f\"{col}_imputed\")\n",
    "    \n",
    "    # Formatar colunas GDP com 5 casas decimais\n",
    "    colunas_gdp = [\"GDP_2012\", \"GDP_2015\", \"GDP_2018\"]\n",
    "    for col in colunas_gdp:\n",
    "        dt_imputed = dt_imputed.withColumn(col, F.round(F.col(col), 5))\n",
    "    \n",
    "    # Arredondar e converter para inteiro outras colunas numéricas\n",
    "    colunas_para_arredondar = [col for col in colunas_numericas if col not in colunas_gdp]\n",
    "    for col in colunas_para_arredondar:\n",
    "        dt_imputed = dt_imputed.withColumn(\n",
    "            col, \n",
    "            F.when(\n",
    "                F.col(col).isNull().cast(\"boolean\"), \n",
    "                F.round(F.col(col), 0).cast(IntegerType())\n",
    "            ).otherwise(F.col(col))\n",
    "        )\n",
    "    \n",
    "    # Contar valores NaN após a imputação\n",
    "    nan_count_after = sum([dt_imputed.filter(F.col(c).isNull()).count() for c in dt_imputed.columns])\n",
    "    print(f\"[PySpark] Valores NaN após: {nan_count_after}\")\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # Parte 4: Adicionar as médias dos anos e das disciplinas\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Criar novas colunas com médias das disciplinas arredondadas a 3 casas decimais\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Means_Maths\",\n",
    "        F.round(F.array_avg(F.array(\"Maths_2012\", \"Maths_2015\", \"Maths_2018\")), 3)\n",
    "    )\n",
    "    \n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Means_Science\",\n",
    "        F.round(F.array_avg(F.array(\"Science_2012\", \"Science_2015\", \"Science_2018\")), 3)\n",
    "    )\n",
    "    \n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Means_Reading\",\n",
    "        F.round(F.array_avg(F.array(\"Reading_2012\", \"Reading_2015\", \"Reading_2018\")), 3)\n",
    "    )\n",
    "    \n",
    "    # Médias por ano (excluindo GDP)\n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Mean_2012\",\n",
    "        F.round(F.array_avg(F.array(\"Maths_2012\", \"Science_2012\", \"Reading_2012\")), 3)\n",
    "    )\n",
    "    \n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Mean_2015\",\n",
    "        F.round(F.array_avg(F.array(\"Maths_2015\", \"Science_2015\", \"Reading_2015\")), 3)\n",
    "    )\n",
    "    \n",
    "    dt_imputed = dt_imputed.withColumn(\n",
    "        \"Mean_2018\",\n",
    "        F.round(F.array_avg(F.array(\"Maths_2018\", \"Science_2018\", \"Reading_2018\")), 3)\n",
    "    )\n",
    "    \n",
    "    # Salvar resultado final\n",
    "    dt_imputed.toPandas().to_excel(\"Dataset_final_pyspark.xlsx\", index=False)\n",
    "    \n",
    "    # Encerrar a sessão Spark\n",
    "    spark.stop()\n",
    "    \n",
    "    return f\"Tempo de execução PySpark: {time.time() - start_time} segundos\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2d6a2eb9-2268-4874-ba32-5f0c273ed733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:27.724346Z",
     "start_time": "2025-05-23T14:58:27.711874Z"
    }
   },
   "source": [
    "run_pandas()\n",
    "\n",
    "run_pandas()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pandas] Valores NaN antes: 70\n",
      "[Pandas] Valores NaN após: 0\n",
      "Tempo de execução Pandas: 0.6846075057983398 segundos\n",
      "\n",
      "[PySpark] Valores NaN antes: 70\n",
      "[PySpark] Valores NaN após: 0\n",
      "Tempo de execução PySpark: 5.5756016855123312 segundos\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
